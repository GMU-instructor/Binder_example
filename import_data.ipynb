{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How do I get data into my notebook?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting file path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Before we start importing, we want to set up our path since it is very long and we don't want to have to \n",
    "# type it repeatedly.\n",
    "from pathlib import Path \n",
    "\n",
    "# Specify path, use 'r' before path to specify 'raw' path or Python will misread backslashes.\n",
    "file_loc = Path(r'C:\\Users\\hruss\\OneDrive\\Documents\\GMU\\Repositories\\Data_files')\n",
    "\n",
    "# Check to see if path is legit\n",
    "print(file_loc.is_dir())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CSVs and Excel files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing CSVs and Excel using Pandas\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, we'll import a CSV.\n",
    "df_csv = pd.read_csv(file_loc/\"Movie_Actors.csv\") \n",
    "df_csv.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Then we'll import an Excel file\n",
    "df_excel = pd.read_excel(file_loc/\"Fortune1000.xlsx\")\n",
    "df_excel.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PDF unstructured text files to wordclouds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now let's bring in some text files\n",
    "from wordcloud import WordCloud\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import PyPDF2 as pyp\n",
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Open PDFs so they can be read, 'rb' means read only in binary format\n",
    "Lincoln_pdf = open(file_loc/'Address_Lincoln.pdf', 'rb')\n",
    "Washington_pdf = open(file_loc/'Address_Washington.pdf', 'rb')\n",
    "Reagan_pdf = open(file_loc/'Address_Reagan.pdf', 'rb')\n",
    "JFK_pdf = open(file_loc/'Address_JFK.pdf', 'rb')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "JFK_read_pdf = pyp.PdfFileReader(JFK_pdf)\n",
    "Lincoln_read_pdf = pyp.PdfFileReader(Lincoln_pdf)\n",
    "Washington_read_pdf = pyp.PdfFileReader(Washington_pdf)\n",
    "Reagan_read_pdf = pyp.PdfFileReader(Reagan_pdf)\n",
    "JFK_data = \"\"\n",
    "Lincoln_data = \"\"\n",
    "Washington_data = \"\"\n",
    "Reagan_data = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Jnum_pages = JFK_read_pdf.numPages\n",
    "for i in range(Jnum_pages) : \n",
    "        Jpage = JFK_read_pdf.getPage(i) \n",
    "        JFK_data = JFK_data + Jpage.extractText()\n",
    "Lnum_pages = Lincoln_read_pdf.numPages\n",
    "for i in range(Lnum_pages) : \n",
    "        Lpage = Lincoln_read_pdf.getPage(i)\n",
    "        Lincoln_data = Lincoln_data + Lpage.extractText()\n",
    "Wnum_pages = Washington_read_pdf.numPages\n",
    "for i in range(Wnum_pages) : \n",
    "        Wpage = Washington_read_pdf.getPage(i)\n",
    "        Washington_data = Washington_data + Wpage.extractText()\n",
    "Rnum_pages = Reagan_read_pdf.numPages\n",
    "for i in range(Rnum_pages) : \n",
    "        Rpage = Reagan_read_pdf.getPage(i)\n",
    "        Reagan_data = Reagan_data + Rpage.extractText()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform NLP on data to create more meaningful word clouds\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "corpus = [Reagan_data, JFK_data, Washington_data, Lincoln_data]\n",
    "\n",
    "# Turn words into vectors\n",
    "vectorizer = TfidfVectorizer(stop_words='english', ngram_range = (1,1), max_df = .6, min_df = .01)\n",
    "X = vectorizer.fit_transform(corpus)\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "dense = X.todense()\n",
    "denselist = dense.tolist()\n",
    "\n",
    "# Turn everything into a dataframe\n",
    "df = pd.DataFrame(denselist, columns=feature_names)\n",
    "data = df.transpose()\n",
    "\n",
    "# Make sure you have these in the same order as your corpus above\n",
    "data.columns = ['Reagan', 'JFK', 'Washington', 'Lincoln']\n",
    "data.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show wordclouds\n",
    "colors = \"viridis\"\n",
    "maxwords = 50\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "# Set overall figure size\n",
    "f = plt.figure(figsize=(12,6))\n",
    "f.tight_layout()\n",
    "\n",
    "# Subplot 1\n",
    "plt.subplot(2, 2, 1)\n",
    "Washington_wordcloud = WordCloud(max_words = maxwords, colormap = colors).generate_from_frequencies(data['Washington'])\n",
    "plt.imshow(Washington_wordcloud)\n",
    "plt.axis('off')\n",
    "plt.title('Washington Speech', fontsize=15)\n",
    "# Subplot 2\n",
    "plt.subplot(2, 2, 2)\n",
    "Reagan_wordcloud = WordCloud(max_words = maxwords, colormap = colors).generate_from_frequencies(data['Reagan'])\n",
    "plt.imshow(Reagan_wordcloud)\n",
    "plt.axis('off')\n",
    "plt.title('Reagan Speech', fontsize=15)\n",
    "# Subplot 3\n",
    "plt.subplot(2, 2, 3)\n",
    "Lincoln_wordcloud = WordCloud(max_words = maxwords, colormap = colors).generate_from_frequencies(data['Lincoln'])\n",
    "plt.imshow(Lincoln_wordcloud)\n",
    "plt.axis('off')\n",
    "plt.title('Lincoln Speech', fontsize=15)\n",
    "# Subplot 4\n",
    "plt.subplot(2, 2, 4)\n",
    "JFK_wordcloud = WordCloud(max_words = maxwords, colormap = colors).generate_from_frequencies(data['JFK'])\n",
    "plt.imshow(JFK_wordcloud)\n",
    "plt.axis('off')\n",
    "plt.title('JFK Speech', fontsize=15);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Images from Github repo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import libraries\n",
    "import os \n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt \n",
    "import matplotlib.image as mpimg \n",
    "import PIL\n",
    "import base64, io, IPython\n",
    "from PIL import Image as Image\n",
    "from urllib.request import urlopen\n",
    "\n",
    "#Pull in files, create folder containing all four files\n",
    "folder = 'https://raw.githubusercontent.com/PacktWorkshops/The-Data-Visualization-Workshop/master/Datasets/images/'\n",
    "files = ['photo-1.jpg', 'photo-2.jpg', 'photo-3.jpg', 'photo-4.jpg']        #How do we automatically iterate over all files in GitHub Folder?\n",
    "imgs = [Image.open(urlopen(os.path.join(folder, file))) for file in files]\n",
    "\n",
    "#Now, let's print them out to look at them\n",
    "fig, axes = plt.subplots(1, 4) \n",
    "fig.dpi = 150\n",
    "labels = ['coast', 'beach', 'building', 'city at night'] \n",
    "for i in range(len(imgs)): \n",
    "    axes[i].imshow(imgs[i]) \n",
    "    axes[i].set_xticks([]) \n",
    "    axes[i].set_yticks([]) \n",
    "    axes[i].set_xlabel(labels[i], color='black')\n",
    "imgarr = np.array(imgs[i])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's say we don't want to keep looking for the images on GitHub. We can embed them into our notebook forever...\n",
    "html = []\n",
    "for i in range(len(imgs)):\n",
    "    output = io.BytesIO()\n",
    "    imgs[i].save(output, format='PNG')\n",
    "    encoded_string = base64.b64encode(output.getvalue()).decode()\n",
    "    html.append('<img src=\"data:image/png;base64,{}\"/>'.format(encoded_string))\n",
    "IPython.display.HTML(html[0])       # Change index from 0 to 1, 2 or 3 to see others"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Special topics in data preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Application Programming Interfaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "# The endpoint URL for joke data API\n",
    "joke_api_url = f\"https://official-joke-api.appspot.com/random_joke\"\n",
    "\n",
    "# Send GET request\n",
    "joke_response = requests.get(joke_api_url)\n",
    "\n",
    "# Assign response to a dataset name\n",
    "joke = joke_response.json()\n",
    "\n",
    "# Print the setup part of the response\n",
    "print(joke['setup'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the punchline part of the response\n",
    "print(joke['punchline'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The endpoint URL for country data API\n",
    "api_url = f\"https://restcountries.com/v3.1/all\"\n",
    "\n",
    "# Send GET request\n",
    "response = requests.get(api_url)\n",
    "\n",
    "data = response.json()\n",
    "\n",
    "# Let's look at one record in the response\n",
    "data[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "countries_df = pd.DataFrame(columns = ['name', 'UN Member', 'Region', 'Lat-Long', 'Population', 'Driving side', 'Start of week'] )\n",
    "\n",
    "for i in data:\n",
    "    countries = \\\n",
    "    [\n",
    "        i['name']['common'],\n",
    "        i['unMember'],\n",
    "    #    i['capital'],\n",
    "        i['region'],\n",
    "        i['latlng'],\n",
    "        i['population'],\n",
    "    #    i['gini'],\n",
    "        i['car']['side'],\n",
    "        i['startOfWeek']\n",
    "    ]\n",
    "    countries_df.loc[len(countries_df)] = countries\n",
    "\n",
    "countries_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text processing / classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "import re\n",
    "import string\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics import accuracy_score, cohen_kappa_score, f1_score, classification_report\n",
    "from sklearn.model_selection import StratifiedKFold, train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = [\n",
    "    \"alt.atheism\",\n",
    "    \"misc.forsale\",\n",
    "    \"sci.space\",\n",
    "    \"soc.religion.christian\",\n",
    "    \"talk.politics.guns\",\n",
    "]\n",
    "\n",
    "news_group_data = fetch_20newsgroups(\n",
    "    subset=\"all\", remove=(\"headers\", \"footers\", \"quotes\"), categories=categories\n",
    ")\n",
    "\n",
    "df = pd.DataFrame(\n",
    "    dict(\n",
    "        text=news_group_data[\"data\"],\n",
    "        target=news_group_data[\"target\"]\n",
    "    )\n",
    ")\n",
    "df[\"target\"] = df.target.map(lambda x: categories[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_text(text):\n",
    "    text = str(text).lower()\n",
    "    text = re.sub(\n",
    "        f\"[{re.escape(string.punctuation)}]\", \" \", text\n",
    "    )\n",
    "    text = \" \".join(text.split())\n",
    "    return text\n",
    "\n",
    "df[\"clean_text\"] = df.text.map(process_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train, df_test = train_test_split(df, test_size=0.20, stratify=df.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec = CountVectorizer(\n",
    "    ngram_range=(1, 3), \n",
    "    stop_words=\"english\",\n",
    ")\n",
    "\n",
    "X_train = vec.fit_transform(df_train.clean_text)\n",
    "X_test = vec.transform(df_test.clean_text)\n",
    "\n",
    "y_train = df_train.target\n",
    "y_test = df_test.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb = MultinomialNB()\n",
    "nb.fit(X_train, y_train)\n",
    "\n",
    "preds = nb.predict(X_test)\n",
    "print(classification_report(y_test, preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joblib.dump(nb, \"nb.joblib\")\n",
    "joblib.dump(vec, \"vec.joblib\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_saved = joblib.load(\"nb.joblib\")\n",
    "vec_saved = joblib.load(\"vec.joblib\")\n",
    "\n",
    "sample_text = [\"Space, Stars, Planets and Astronomy!\"]\n",
    "# Process the text in the same way you did when you trained it!\n",
    "clean_sample_text = process_text(sample_text)\n",
    "sample_vec = vec_saved.transform(sample_text)\n",
    "nb_saved.predict(sample_vec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Image processing / classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The following code comes from here: https://pytorch.org/tutorials/beginner/blitz/cifar10_tutorial.html \n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "# Assuming that we are on a CUDA machine, this should print a CUDA device:\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "batch_size = 4\n",
    "\n",
    "trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
    "\n",
    "testset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size, shuffle=False, num_workers=2)\n",
    "\n",
    "classes = ('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# get some random training images\n",
    "dataiter = iter(trainloader)\n",
    "images, labels = next(dataiter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = torchvision.utils.make_grid(images)\n",
    "npimg = img.numpy() # normalized image\n",
    "plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "plt.show()\n",
    "print(' '.join(f'{classes[labels[j]]:5s}' for j in range(batch_size)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = torchvision.utils.make_grid(images)\n",
    "img = img / 2 # unnormalize to make image easier to view\n",
    "npimg = img.numpy()\n",
    "plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "plt.show()\n",
    "print(' '.join(f'{classes[labels[j]]:5s}' for j in range(batch_size)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = torchvision.utils.make_grid(images)\n",
    "img = img / 2 + 0.25 # unnormalize to make image easier to view\n",
    "npimg = img.numpy()\n",
    "plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "plt.show()\n",
    "print(' '.join(f'{classes[labels[j]]:5s}' for j in range(batch_size)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = torchvision.utils.make_grid(images)\n",
    "img = img / 2 + 0.5 # unnormalize to make image easier to view\n",
    "npimg = img.numpy()\n",
    "plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "plt.show()\n",
    "print(' '.join(f'{classes[labels[j]]:5s}' for j in range(batch_size)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = torchvision.utils.make_grid(images)\n",
    "img = img / 2 + 0.75 # unnormalize to make image easier to view\n",
    "npimg = img.numpy()\n",
    "plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "plt.show()\n",
    "print(' '.join(f'{classes[labels[j]]:5s}' for j in range(batch_size)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = torchvision.utils.make_grid(images)\n",
    "img = img / 3 + 0.75 # unnormalize to make image easier to view\n",
    "npimg = img.numpy()\n",
    "plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "plt.show()\n",
    "print(' '.join(f'{classes[labels[j]]:5s}' for j in range(batch_size)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 6, 5)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = torch.flatten(x, 1) # flatten all dimensions except batch\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "net = Net()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(2):  # loop over the dataset multiple times\n",
    "\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        # get the inputs; data is a list of [inputs, labels]\n",
    "        inputs, labels = data\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "        if i % 2000 == 1999:    # print every 2000 mini-batches\n",
    "            print(f'[{epoch + 1}, {i + 1:5d}] loss: {running_loss / 2000:.3f}')\n",
    "            running_loss = 0.0\n",
    "\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = './cifar_net.pth'\n",
    "torch.save(net.state_dict(), PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataiter = iter(testloader)\n",
    "images, labels = next(dataiter)\n",
    "\n",
    "# print images\n",
    "img = torchvision.utils.make_grid(images)\n",
    "img = img / 2 + 0.35 # unnormalize to make image easier to view\n",
    "npimg = img.numpy()\n",
    "plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "plt.show()\n",
    "print('GroundTruth: ', ' '.join(f'{classes[labels[j]]:5s}' for j in range(4)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = Net()\n",
    "net.load_state_dict(torch.load(PATH))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, predicted = torch.max(outputs, 1)\n",
    "\n",
    "print('Predicted: ', ' '.join(f'{classes[predicted[j]]:5s}'\n",
    "                              for j in range(4)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "# since we're not training, we don't need to calculate the gradients for our outputs\n",
    "with torch.no_grad():\n",
    "    for data in testloader:\n",
    "        images, labels = data\n",
    "        # calculate outputs by running images through the network\n",
    "        outputs = net(images)\n",
    "        # the class with the highest energy is what we choose as prediction\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "print(f'Accuracy of the network on the 10000 test images: {100 * correct // total} %')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare to count predictions for each class\n",
    "correct_pred = {classname: 0 for classname in classes}\n",
    "total_pred = {classname: 0 for classname in classes}\n",
    "\n",
    "# again no gradients needed\n",
    "with torch.no_grad():\n",
    "    for data in testloader:\n",
    "        images, labels = data\n",
    "        outputs = net(images)\n",
    "        _, predictions = torch.max(outputs, 1)\n",
    "        # collect the correct predictions for each class\n",
    "        for label, prediction in zip(labels, predictions):\n",
    "            if label == prediction:\n",
    "                correct_pred[classes[label]] += 1\n",
    "            total_pred[classes[label]] += 1\n",
    "\n",
    "\n",
    "# print accuracy for each class\n",
    "for classname, correct_count in correct_pred.items():\n",
    "    accuracy = 100 * float(correct_count) / total_pred[classname]\n",
    "    print(f'Accuracy for class: {classname:5s} is {accuracy:.1f} %')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "absl-py==1.3.0\n",
      "affine==2.3.1\n",
      "aiohttp==3.8.3\n",
      "aiosignal==1.3.1\n",
      "ale-py==0.8.0\n",
      "altair==4.2.0\n",
      "ansi2html==1.8.0\n",
      "anyascii==0.3.2\n",
      "anyio==3.6.2\n",
      "appdirs==1.4.4\n",
      "apptools==5.2.0\n",
      "argon2-cffi==21.3.0\n",
      "argon2-cffi-bindings==21.2.0\n",
      "arrow==1.2.3\n",
      "artist==0.18.2\n",
      "asttokens==2.0.8\n",
      "astunparse==1.6.3\n",
      "async-timeout==4.0.2\n",
      "attrs==22.1.0\n",
      "audioread==3.0.0\n",
      "autobahn==22.12.1\n",
      "backcall==0.2.0\n",
      "beautifulsoup4==4.11.1\n",
      "binaryornot==0.4.4\n",
      "bleach==5.0.1\n",
      "blinker==1.5\n",
      "blis==0.7.9\n",
      "bokeh==2.4.3\n",
      "branca==0.6.0\n",
      "Brotli==1.0.9\n",
      "cachetools==5.2.0\n",
      "catalogue==2.0.8\n",
      "celluloid==0.2.0\n",
      "certifi==2022.9.24\n",
      "cffi==1.15.1\n",
      "chardet==5.0.0\n",
      "charset-normalizer==2.1.1\n",
      "chart-studio==1.1.0\n",
      "click==8.1.3\n",
      "click-default-group==1.2.2\n",
      "click-plugins==1.1.1\n",
      "cligj==0.7.2\n",
      "cloudpickle==2.2.0\n",
      "cloup==0.13.1\n",
      "clustergram==0.6.0\n",
      "clusteval==2.1.4\n",
      "cmake==3.24.1.1\n",
      "cmdstanpy==1.2.0\n",
      "cmp==0.0.1\n",
      "cmyt==1.1.3\n",
      "codecarbon==2.3.1\n",
      "colorama==0.4.6\n",
      "colorcet==3.0.1\n",
      "colorlover==0.3.0\n",
      "colorspacious==1.1.2\n",
      "colour==0.1.5\n",
      "colourmap==1.1.9\n",
      "commonmark==0.9.1\n",
      "confection==0.0.4\n",
      "ConfigArgParse==1.5.3\n",
      "configobj==5.0.6\n",
      "contextily==1.2.0\n",
      "contourpy==1.0.5\n",
      "contractions==0.1.73\n",
      "cookiecutter==2.1.1\n",
      "cpi==1.0.22\n",
      "cryptography==39.0.0\n",
      "cufflinks==0.17.3\n",
      "cycler==0.11.0\n",
      "cymem==2.0.7\n",
      "dash==2.6.2\n",
      "dash-core-components==2.0.0\n",
      "dash-html-components==2.0.0\n",
      "dash-table==5.0.0\n",
      "debugpy==1.6.3\n",
      "decorator==5.1.1\n",
      "defusedxml==0.7.1\n",
      "Deprecated==1.2.13\n",
      "discopy==0.5.0\n",
      "distlib==0.3.6\n",
      "docopt==0.6.2\n",
      "elevation==1.1.3\n",
      "emoji==2.2.0\n",
      "en-core-web-sm @ https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.5.0/en_core_web_sm-3.5.0-py3-none-any.whl\n",
      "entrypoints==0.4\n",
      "enum34==1.1.10\n",
      "envisage==6.1.0\n",
      "et-xmlfile==1.1.0\n",
      "exceptiongroup==1.0.0rc9\n",
      "executing==1.1.0\n",
      "fasteners==0.18\n",
      "fastjsonschema==2.16.2\n",
      "filelock==3.8.0\n",
      "Fiona==1.8.22\n",
      "Flask==2.2.2\n",
      "Flask-Compress==1.13\n",
      "flatbuffers==23.5.26\n",
      "folium==0.13.0\n",
      "fonttools==4.37.4\n",
      "frozenlist==1.3.3\n",
      "funcy==2.0\n",
      "future==0.18.3\n",
      "fuzzywuzzy==0.18.0\n",
      "gast==0.4.0\n",
      "gensim==4.3.1\n",
      "geographiclib==1.52\n",
      "geopandas==0.12.1\n",
      "geopy==2.2.0\n",
      "gitdb==4.0.9\n",
      "GitPython==3.1.29\n",
      "glcontext==2.3.6\n",
      "glfw==1.12.0\n",
      "google-auth==2.23.0\n",
      "google-auth-oauthlib==1.0.0\n",
      "google-pasta==0.2.0\n",
      "gr==1.21.0\n",
      "graphviz==0.20.1\n",
      "grpcio==1.50.0\n",
      "gym==0.26.2\n",
      "gym-notices==0.0.8\n",
      "gym3==0.3.3\n",
      "h5py==3.7.0\n",
      "holidays==0.37\n",
      "hyperlink==21.0.0\n",
      "idna==3.4\n",
      "imageio==2.31.3\n",
      "imageio-ffmpeg==0.3.0\n",
      "import-ipynb==0.1.4\n",
      "importlib-metadata==5.0.0\n",
      "importlib-resources==5.10.0\n",
      "inflection==0.5.1\n",
      "iniconfig==1.1.1\n",
      "investpy==1.0.8\n",
      "ipycanvas==0.13.1\n",
      "ipydatawidgets==4.3.2\n",
      "ipyevents==2.0.1\n",
      "ipygany==0.5.0\n",
      "ipykernel==6.16.0\n",
      "ipympl==0.9.2\n",
      "ipython==8.5.0\n",
      "ipython-genutils==0.2.0\n",
      "ipyvolume==0.5.2\n",
      "ipyvtklink==0.2.3\n",
      "ipywebrtc==0.6.0\n",
      "ipywidgets==7.7.2\n",
      "isosurfaces==0.1.0\n",
      "itk-core==5.2.1.post1\n",
      "itk-filtering==5.2.1.post1\n",
      "itk-meshtopolydata==0.8.1\n",
      "itk-numerics==5.2.1.post1\n",
      "itkwidgets==0.32.3\n",
      "itsdangerous==2.1.2\n",
      "jedi==0.18.1\n",
      "Jinja2==3.1.2\n",
      "jinja2-time==0.2.0\n",
      "joblib==1.2.0\n",
      "jsonpickle==3.0.2\n",
      "jsonschema==4.16.0\n",
      "jupyter==1.0.0\n",
      "jupyter-bokeh==3.0.5\n",
      "jupyter-console==6.4.4\n",
      "jupyter-core==4.11.1\n",
      "jupyter-dash==0.4.2\n",
      "jupyter-server==1.23.2\n",
      "jupyter-server-mathjax==0.2.6\n",
      "jupyter-server-proxy==3.2.2\n",
      "jupyter_client==7.3.5\n",
      "jupyterlab-pygments==0.2.2\n",
      "jupyterlab-widgets==1.1.3\n",
      "k3d==2.15.2\n",
      "keras==2.13.1\n",
      "Keras-Preprocessing==1.1.2\n",
      "kiwisolver==1.4.4\n",
      "kneed==0.8.1\n",
      "langcodes==3.3.0\n",
      "lazy_loader==0.3\n",
      "libclang==14.0.6\n",
      "librosa==0.10.1\n",
      "llvmlite==0.40.1\n",
      "lxml==4.9.2\n",
      "manim==0.17.2\n",
      "ManimPango==0.4.3\n",
      "mapbox-earcut==1.0.1\n",
      "Markdown==3.4.1\n",
      "MarkupSafe==2.1.1\n",
      "matplotlib==3.6.2\n",
      "matplotlib-inline==0.1.6\n",
      "matplotlib-label-lines==0.5.1\n",
      "mayavi==4.8.1\n",
      "mercantile==1.2.1\n",
      "Mesa==1.1.0\n",
      "mir-eval==0.7\n",
      "mistune==2.0.4\n",
      "mlxtend==0.21.0\n",
      "moderngl==5.6.4\n",
      "moderngl-window==2.4.2\n",
      "more-itertools==9.0.0\n",
      "mpmath==1.2.1\n",
      "msgpack==1.0.4\n",
      "multidict==6.0.2\n",
      "multipledispatch==0.6.0\n",
      "munch==2.5.0\n",
      "murmurhash==1.0.9\n",
      "mylibapp==0.1.0\n",
      "mystyles==0.1.0\n",
      "names-dataset==2.1.0\n",
      "Nasdaq-Data-Link==1.0.4\n",
      "nbclassic==0.4.8\n",
      "nbclient==0.7.0\n",
      "nbconvert==7.2.5\n",
      "nbdime==3.1.1\n",
      "nbformat==5.5.0\n",
      "nbmerge==0.0.4\n",
      "nest-asyncio==1.5.6\n",
      "networkx==2.8.7\n",
      "nltk==3.8.1\n",
      "notebook==6.5.2\n",
      "notebook_shim==0.2.2\n",
      "numba==0.57.1\n",
      "numexpr==2.8.4\n",
      "numpy==1.24.3\n",
      "oauthlib==3.2.2\n",
      "open3d==0.16.0\n",
      "openai==0.27.8\n",
      "opencv-python==4.8.0.76\n",
      "openpyxl==3.0.10\n",
      "opt-einsum==3.3.0\n",
      "osmnx==1.6.0\n",
      "packaging==21.3\n",
      "pandas==2.0.1\n",
      "pandas-ml==0.6.1\n",
      "pandocfilters==1.5.0\n",
      "panel==0.14.1\n",
      "param==1.12.2\n",
      "parso==0.8.3\n",
      "pathy==0.10.1\n",
      "patsy==0.5.3\n",
      "pca==1.8.3\n",
      "pickleshare==0.7.5\n",
      "Pillow==9.2.0\n",
      "pip-chill==1.0.1\n",
      "pipreqs==0.4.13\n",
      "pipreqsnb==0.2.4\n",
      "platformdirs==2.5.4\n",
      "plotly==5.10.0\n",
      "plotoptix==0.14.4\n",
      "pluggy==1.0.0\n",
      "pooch==1.6.0\n",
      "preshed==3.0.8\n",
      "prettytable==3.9.0\n",
      "procgen==0.10.7\n",
      "prometheus-client==0.15.0\n",
      "prompt-toolkit==3.0.31\n",
      "prophet==1.1.5\n",
      "protobuf==4.24.3\n",
      "psutil==5.9.2\n",
      "pure-eval==0.2.2\n",
      "py-cpuinfo==9.0.0\n",
      "pyahocorasick==2.0.0\n",
      "pyarrow==10.0.0\n",
      "pyasn1==0.4.8\n",
      "pyasn1-modules==0.2.8\n",
      "pycairo==1.23.0\n",
      "pycparser==2.21\n",
      "pyct==0.4.8\n",
      "pydantic==1.10.7\n",
      "pydeck==0.8.0\n",
      "pydotplus==2.0.2\n",
      "pydub==0.25.1\n",
      "pyface==7.4.4\n",
      "pygame==2.1.0\n",
      "pyglet==2.0.3\n",
      "Pygments==2.13.0\n",
      "pyLDAvis==3.4.1\n",
      "Pympler==1.0.1\n",
      "pynvml==11.5.0\n",
      "pyparsing==3.0.9\n",
      "PyPDF2==2.11.1\n",
      "pyperclip==1.8.2\n",
      "pyphen==0.14.0\n",
      "pypickle==1.1.0\n",
      "pyproj==3.4.0\n",
      "PyQt5==5.15.7\n",
      "PyQt5-Qt5==5.15.2\n",
      "PyQt5-sip==12.11.0\n",
      "pyRAPL==0.2.3.1\n",
      "pyrr==0.10.3\n",
      "pyrsistent==0.18.1\n",
      "pyrunner==0.1.4\n",
      "pyspellchecker==0.7.1\n",
      "pytest==7.2.0\n",
      "python-dateutil==2.8.2\n",
      "python-slugify==6.1.2\n",
      "pythreejs==2.4.1\n",
      "pytz==2022.4\n",
      "pytz-deprecation-shim==0.1.0.post0\n",
      "pyvista==0.37.0\n",
      "pyviz-comms==2.2.1\n",
      "PyWavelets==1.4.1\n",
      "pywin32==304\n",
      "pywinpty==2.0.9\n",
      "PyYAML==6.0\n",
      "pyzmq==24.0.1\n",
      "qtconsole==5.4.0\n",
      "QtPy==2.3.0\n",
      "Quandl==3.7.0\n",
      "quasardb==3.13.7\n",
      "rasterio==1.3.3\n",
      "rdkit==2022.9.3\n",
      "regex==2023.3.23\n",
      "requests==2.28.1\n",
      "requests-oauthlib==1.3.1\n",
      "retrying==1.3.3\n",
      "rich==12.6.0\n",
      "rsa==4.9\n",
      "sacred==0.8.5\n",
      "safetensors==0.3.3\n",
      "scatterd==1.2.0\n",
      "scikit-image==0.21.0\n",
      "scikit-learn==1.1.2\n",
      "scipy==1.9.1\n",
      "scooby==0.7.0\n",
      "screeninfo==0.8.1\n",
      "seaborn==0.13.0\n",
      "semver==2.13.0\n",
      "Send2Trash==1.8.0\n",
      "session-info==1.0.0\n",
      "shapely==2.0.1\n",
      "simpervisor==0.4\n",
      "simpy==4.0.1\n",
      "six==1.16.0\n",
      "skia-pathops==0.7.4\n",
      "sklearn==0.0\n",
      "smart-open==6.3.0\n",
      "smmap==5.0.0\n",
      "sniffio==1.3.0\n",
      "snuggs==1.4.7\n",
      "soundfile==0.12.1\n",
      "soupsieve==2.3.2.post1\n",
      "soxr==0.3.6\n",
      "spacy==3.5.2\n",
      "spacy-legacy==3.0.12\n",
      "spacy-loggers==1.0.4\n",
      "srsly==2.4.6\n",
      "srt==3.5.2\n",
      "stack-data==0.5.1\n",
      "stanio==0.3.0\n",
      "statsmodels==0.14.0\n",
      "stdlib-list==0.10.0\n",
      "streamlit==1.14.0\n",
      "svgelements==1.9.0\n",
      "sympy==1.11.1\n",
      "tenacity==8.1.0\n",
      "tensorboard==2.13.0\n",
      "tensorboard-data-server==0.7.1\n",
      "tensorboard-plugin-wit==1.8.1\n",
      "tensorflow==2.13.0\n",
      "tensorflow-estimator==2.13.0\n",
      "tensorflow-intel==2.13.0\n",
      "tensorflow-io-gcs-filesystem==0.27.0\n",
      "termcolor==2.1.0\n",
      "terminado==0.17.0\n",
      "text-preprocessing==0.1.1\n",
      "text-unidecode==1.3\n",
      "textblob==0.17.1\n",
      "textsearch==0.0.24\n",
      "textstat==0.7.3\n",
      "thinc==8.1.9\n",
      "threadpoolctl==3.1.0\n",
      "tifffile==2023.8.30\n",
      "tiktoken==0.4.0\n",
      "tile==1.0.0a9\n",
      "tinycss2==1.2.1\n",
      "tokenizers==0.13.3\n",
      "toml==0.10.2\n",
      "tomli==2.0.1\n",
      "tomli_w==1.0.0\n",
      "toolz==0.12.0\n",
      "torch==2.0.1\n",
      "torchvision==0.15.2\n",
      "tornado==6.2\n",
      "tqdm==4.64.1\n",
      "traitlets==5.4.0\n",
      "traits==6.4.1\n",
      "traitsui==7.4.2\n",
      "traittypes==0.2.1\n",
      "txaio==22.2.1\n",
      "typer==0.7.0\n",
      "typing_extensions==4.4.0\n",
      "tzdata==2022.6\n",
      "tzlocal==4.2\n",
      "Unidecode==1.3.6\n",
      "unittest-xml-reporting==3.2.0\n",
      "unyt==2.9.3\n",
      "urbangrammar-graphics==1.2.3\n",
      "urllib3==1.26.12\n",
      "us-federal-treasury-python-api==0.1.0\n",
      "validators==0.20.0\n",
      "vedo==2022.4.1\n",
      "virtualenv==20.16.7\n",
      "vpython==7.6.4\n",
      "vtk==9.2.2\n",
      "wasabi==1.1.1\n",
      "watchdog==2.1.9\n",
      "wcwidth==0.2.5\n",
      "webencodings==0.5.1\n",
      "websocket-client==1.4.2\n",
      "Werkzeug==2.2.2\n",
      "wget==3.2\n",
      "widgetsnbextension==3.6.3\n",
      "wordcloud==1.8.2.2\n",
      "wrapt==1.14.1\n",
      "wslink==1.9.1\n",
      "xgboost==1.7.2\n",
      "xlrd==2.0.1\n",
      "xyzservices==2022.9.0\n",
      "yarg==0.1.9\n",
      "yarl==1.8.1\n",
      "yt==4.1.3\n",
      "zipp==3.9.0\n",
      "zstandard==0.19.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\hruss\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages)\n"
     ]
    }
   ],
   "source": [
    "! pip freeze"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 64-bit (microsoft store)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "7edd4e905d9b7ca730f7b0e9c082715bdac78ae21b6c7d0ae05faf51bf553083"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
